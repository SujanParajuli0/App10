{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228a5dc0",
   "metadata": {},
   "source": [
    "## Deep Learning Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d14c14",
   "metadata": {},
   "source": [
    "## In this assignment, you will be a pixel-based neural network for (multi-class) classification as well as learn how to expand your neural network modeling skills into the spatial realm with convolutional neural networks (chip-based method) and how you can apply convolutional neural network to pixel based classification tasks (semantic segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d0a99",
   "metadata": {},
   "source": [
    "### Raster Files:\n",
    "- Landsat.tif ( remotely sensed data in the form of surface reflectance, will be used as the input to our model)[Values (0 - 10000)] numerical\n",
    "- Impervious.tif (NLCD fractional impervious map, will be used as our \"ground truth\" in training some of our model) [values ( 0 - 100)] numerical\n",
    "- Dem.tif(ancilliary data in the form of elevation data) [values (0 - 10000)] numerical\n",
    "- Aspect.tif ( anciliary data in the form of downlslope direction) [values (0-17)] categorical\n",
    "- Posidex.tif ( ancillary data in the form of positional index)[values (0-100)] numerical\n",
    "- \n",
    "wetlands.tif (ancillary data in the form of wetlands information)[values (0 - 8)]categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6799e",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4649cafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2w/bg5rznk95pzgwm04qhnrdt7w0000gn/T/ipykernel_6680/3355039531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8fea5",
   "metadata": {},
   "source": [
    "## 1a. Import your mulit-class classification preprocessing function\n",
    "Hint: see instrucitons for example function if you do not have one written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b97eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import multiclass_classification_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a22bfd",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ec019",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"./data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4cbe01",
   "metadata": {},
   "source": [
    "### 1c. Expand this function to create a nn_multiclass_classification_preprocessing function that scales all continuous values 0 -1\n",
    "Hint: Neural network expects normalized data, if you know a global scaling value, scale by that, if not just perform z-score scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8ef9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_multiclass_classification_preprocess(dataframe):\n",
    "    dataset = multiclass_classification_preprocess(dataframe)\n",
    "    dataset[['landsat_1', 'landsat_2', 'landsat_3', \n",
    "             'landsat_4', 'landsat_5', 'landsat_6', 'dem_1']] = dataset[['landsat_1', 'landsat_2', 'landsat_3', \n",
    "             'landsat_4', 'landsat_5', 'landsat_6', 'dem_1']] / 10000\n",
    "    dataset['NDVI'] = (dataset['landsat_4'] - dataset['landsat_3']) / (dataset['landsat_4'] + dataset['landsat_3'])\n",
    "    dataset['posidex_1'] = dataset['posidex_1'] / 100\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nn_multiclass_classification_preprocess(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03c845",
   "metadata": {},
   "source": [
    "## Print dataset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de98926",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6517d3",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'landcover_1'\n",
    "features = ['landsat_1', 'landsat_2', 'landsat_3', 'landsat_4', 'landsat_5', 'landsat_6',\n",
    "            'aspect_1_0','aspect_1_1','aspect_1_2','aspect_1_3','aspect_1_4','wetlands_1_0','wetlands_1_1','wetlands_1_2'\n",
    "            ,'dem_1','posidex_1','NDVI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd6d94",
   "metadata": {},
   "source": [
    "### 2.a. Assign the target feature variables\n",
    "- Assign \"landcover_1\" as the target variable called target and features = ['landsat{i}', 'dem_1', 'posidex_1', 'aspect1{i}','wetlands1{i}']\n",
    "- Recap : Target variable is the dependent variable. That is, the variable whose value we want to predict\n",
    "- Feature variable is the indepedent variable i.e. the variable which are used to predict the value of the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[features]\n",
    "y = dataset[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c0ae3",
   "metadata": {},
   "source": [
    "## 2.a. Print unique values in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ba736",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecccd27",
   "metadata": {},
   "source": [
    "## 3. Label Encoding\n",
    "Note: NLCD class legend \n",
    "- We need to remap these values to be between 0 - num_classes\n",
    "-  In label encoding, each label is assigned a unique integer. This is based on alphabetical ordering For example, if you were to encode Apple, strawberry, and orange, apple would be assigned the value 0 , orange:1,and strawberry:2\n",
    "- you can use packaged solutions to this problem or create your own solution like this below\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {11:0, 12:1, 21:2, 22:3, 23: 4, 24:5, 31:6, 41:7, 42:8,\n",
    "                 43:9, 52:10, 71:11, 81:12, 82: 13, 90:14, 95: 15}\n",
    "def remap(value: int, encoding: dict) -> int:\n",
    "    return encoding[value]\n",
    "\n",
    "# Encode labels in column 'landcover_1'\n",
    "y = np.vectorize(lambda:x label_encoding[int(x)])(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714ab04",
   "metadata": {},
   "source": [
    "## 3.a. Print unique values in encoded y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe952705",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3f964",
   "metadata": {},
   "source": [
    "## 3.b. Sparse categorical crossentropy vs categorical crossentropy\n",
    "Categorical crossentropy expects a vector of one-hot encoded labels where as sparse categorical crossentropy expects a vector of 0-num_classes labels. So, if we want to use the y vector as is, we would need to use sparse categorical crossentropy. If we want to use categorical crossentropy, we would need to one-hot encode the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=len(label_encoding.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8fb06",
   "metadata": {},
   "source": [
    "## 3.d. Print X, y shapes for model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85487d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8a191",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f323bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df253f",
   "metadata": {},
   "source": [
    "## 4. Building the model\n",
    "Note: Here we are building a pixel-based neural network, the last dense layer outputs to the number of classes we have and since this is a multi-class classification problem, the final activation is softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed141dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional\n",
    "def dense_block(x , nlayers, dropout=0,batch_norm=False):\n",
    "    \n",
    "    x = tf.keras.layers.Dense(nlayers, activation='relu')(x)\n",
    "    if dropout > 0:\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    if batch_norm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def DNN_functional(shape, out_dim, activ, norm=False):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape)\n",
    "    \n",
    "    x = dense_block(inputs, 64, 0, norm)\n",
    "    x = dense_block(x, 128, 0, norm)\n",
    "    x = dense_block(x, 1024, 0.5, norm)\n",
    "    x = dense_block(x, 256, 0.2, norm)\n",
    "    output = tf.keras.layers.Dense(out_dim, activation=activ)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=output, name='DNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98b302",
   "metadata": {},
   "source": [
    "## 4.a. Create model variable and Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_functional(36, len(label_encoding.keys()), 'softmax', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7c0e5",
   "metadata": {},
   "source": [
    "## 5. Training the neural network\n",
    "Steps to follow:\n",
    "- Wrangle Training / validation data\n",
    "- Preprocess Data for Neural Network\n",
    "\n",
    "Note: input data needs to be either between [-1, 1] for neural networks, so categorical data must be one-hot encoded, continuous data normalized\n",
    "- Build network\n",
    "- Compile Network with specified hyperparameters\n",
    "- Train the network\n",
    "- Iterate\n",
    "\n",
    "Note: If you haven't yet, add the run_training and compile_model functions from week 8 to your functions.py file sothat you can import them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import run_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622ce5e",
   "metadata": {},
   "source": [
    "## 5.a. Call the training functiona and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb53749",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_training(model, (X_train, y_train), (X_test, y_test), 10, 1024, tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "                      'categorical_crossentropy', 'categorical_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74664d59",
   "metadata": {},
   "source": [
    "## 5.b. Convert the history.history dictionary to a dataframe called \"learning history\" and print dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_history = pd.DataFrame(history.history)\n",
    "learning_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecff863",
   "metadata": {},
   "source": [
    "## 5.c. Print a line plot that shows the categorical_accuracy and val_categorical_accuracy\n",
    "\n",
    "With the help of this plot, you can see how the accuracy increases with the increase in number of epochs\n",
    "Hint: You can find the code in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b17314",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_history.plot.line(y=['categorical_accuracy', 'val_categorical_accuracy'], ylim=(0.5, 1), figsize=(15, 6),\n",
    "                          xticks=np.arange(0, 31, 1), yticks= np.arange(0, 1.1, 0.1), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cfbfe",
   "metadata": {},
   "source": [
    "## 6. Evaluate the model\n",
    " 6.a. Load in Checkpoint model\n",
    " Hint: Look at instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aceb13f",
   "metadata": {},
   "source": [
    "## 6.b. Predict the observations for train and test sets and assign the values to X_train_pred and X_test_pred resp. and print two variables shape (X_train_pred and X_test_pred)\n",
    "\n",
    "How did this iteration of the model perform?\n",
    "\n",
    "Try adding in one more hidden layer, increasing the number of nodes in each hidden layer, trying different activation functions etc.\n",
    "\n",
    "Do some hyperparamter tuning and see if you can create a model that performs even better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = model.predict(X_train)\n",
    "x_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b601894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_pred.shape)\n",
    "print(X_test_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8677a3",
   "metadata": {},
   "source": [
    "### 5.b. Return indices of the maximum values in X_train_pred and X_test_pred and assign to variables tr_max_indics and te_max_indices resp.\n",
    "\n",
    "Note: these aere softmax outputs, this mean we need to take the argmax to determine which class the maximum prediction belongs to \n",
    "\n",
    "Hint: np.argmax(, axis=softmax_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72318afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_max_indices = np.argmax(X_train_pred, axis = -1)\n",
    "te_max_indices = np.argmax(X_test_pred, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357b5a6",
   "metadata": {},
   "source": [
    "### Return indices of the maximum values in y_train and y_test and assign them to variables ytr_max_indices and yte_max_indices resp\n",
    "\n",
    "Note: These are softmax outputs, this means we need to take the argmax to determine which class the maximum prediction belong to\n",
    "\n",
    "HINT: np.argmax(,axis=softmax_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67baa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_max_indices = np.argmax(y_train, axis=-1)\n",
    "yte_max_indices = np.argmax(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3dc3c",
   "metadata": {},
   "source": [
    "### 6.b. Print the accuracy, F1 score and classification report for the train set\n",
    "\n",
    "Hint: you can find the code in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb534f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'water', 1:'snow',2:'open_space',3:'low_urban',4:'med_urban',5:'high_urban',6:'barren',\n",
    "         7:'d_forest',8:'e_forest',9:'m_forest',10:'shrub',11:'grass',12:'hay',13:'crops',14:'w_wetlands',\n",
    "         15:'e_wetlands'}\n",
    "labels = list(labels.values())\n",
    "labels.remove(snow)\n",
    "print(f'Accuracy  :{accuracy_score(ytr_max_indices, tr_max_indices)}')\n",
    "print(f'f1-score :{f1_score(ytr_max_indices, tr_max_indices, average=\"weighted\")}')\n",
    "print(f'Report  :\\n{classification_report(ytr_max_indices, tr_max_indices, target_names=labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdfaaa",
   "metadata": {},
   "source": [
    "## Part 2: Chip Based Method (CNN >- Unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f103b",
   "metadata": {},
   "source": [
    "### 1. Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5183f7",
   "metadata": {},
   "source": [
    "### 1.a. Pixel Based\n",
    "\n",
    "Up until this point we have been using point based methods, methods where we sample individual points of data. below you will see a visual representation of this with the green dot being the training and the red dots being the validation (test) points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b05ad5",
   "metadata": {},
   "source": [
    "#### Also, it is important to note that these points based methos result in 2 dimensional data. Let's take a look at the shape of our training dataset to see what this looks like.\n",
    "\n",
    "\n",
    "NOTE: the shape represents (samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b26653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de226a5",
   "metadata": {},
   "source": [
    "### 1b. Chip Based\n",
    "\n",
    "\n",
    "Here you can see we are now sampling chips fof data of ROIs (Region of interest). The gree here are also training and the ered validation (test0. When creating this chips of data, you want to create them with the same number of reow and number of columns and you also want to make sure the number yo chose is a power of 2. This is because the Unet convolutional neural network essentially performs down and upsampling while processing the layers and it expects chips such as (512x512),(256x256), (128x128), etc.\n",
    "\n",
    "NOTE: Chosing your chip size is a hyperparameter of your system and cam make some difference, For geospatial data, i like to keep in mind that the model should have acccess to some global context when making local decisions, but not too much context.Therefore, we need to tak into consideration the resolution of the imagery (how many meters are represented by one pixel) when determining our chip size. I have had good luck with 256x256 for building Unets for Landsat Imagery with a 30m resolution.\n",
    "\n",
    "\n",
    "Let's take a look at what the dataset might look like for chip-based data. Let's load in a couple chips of our landsat data using the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ce95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "\n",
    "def read_image(path: str, xgeo: int, yeo: int, ncols:int, nrows:int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read raster window into numpy array\n",
    "    \"\"\"\n",
    "    with rio.open(path) as ds:\n",
    "        row, col = ds.index(xgeo, ygeo)\n",
    "        data = ds.read(\n",
    "        window=rio.windows.window(col, row, ncols, nrows))\n",
    "        data[data==ds.nodata] = 0\n",
    "    return data.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"landsat\": './data/landsat_2019.tif' # pure raw data from satellite\n",
    "    \"landcover\": './data/landcover_2019.tif' # modeled/mapped (classification)\n",
    "}\n",
    "\n",
    "coords = [\n",
    "    (1034416, 1364804),\n",
    "    (1042095, 1357125)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed14d2",
   "metadata": {},
   "source": [
    "Note: if we generate an array of samples with a single chanel raster, in this case the \"landcover\" raster end up with a 3-dimensional array. the output being (samples, height, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([read_image(paths['landsat'], *coord, 256, 256) for coord in coords]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58bf26",
   "metadata": {},
   "source": [
    "Note: Tensorflow is in \"channels last\" mode by default. This means it expects the inputs to be (samples, height, weight, channels) so we will have to move the axis of these data to be in the proper orientation\n",
    "\n",
    "IMPORTANT: When creating your datasets, if you need to transpose your input imagery make sure to also transpose your target masks so that they line up with each other. Use matplotlib to visualize to ensure everything is lning up properly before scaling up\n",
    "\n",
    "NOTE: Pytorch is in \"channel first\" mode by default, So no transposition is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([read_image(paths['landsat'], *coord, 256, 256).transpose() for coord in coords]).shape # Channels last make sure to orient the mask as well when reading this into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b37935",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n",
    "ax[0].imshow(np.array([read_image(paths['landsat'], *coord, 256, 256).transpose() for coord in coords])[0, :, : ,0])\n",
    "ax[1].imshow(np.array([read_image(paths['landcover'], *coord, 256, 256).transpose() for coord in coords])[0, :, : ,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac155209",
   "metadata": {},
   "source": [
    "## 2. Batch loading\n",
    "\n",
    "When you start to move into larger datasets you're not going to be able to load everything into memory of all at once like weh have been you will have to start performing batch loading and batch processing which is where using tensorflow td.data.Dataset pipelines and pytorch dataloaders start to come in handy. One approach is to create a csv of ROI information that acts as the \"dataset\" then when you need a batch of actual raster data, you define a dataset loading function that takes the batch of ROI information and read those data chips into memory for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f686e",
   "metadata": {},
   "source": [
    "### 2.a. Creating CSV of ROI information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chips(paths, bounds, chip=256, gsd=30):\n",
    "    \"\"\"\n",
    "    Generates grid of chips for training data\n",
    "    \"\"\"\n",
    "    total_chunks = []\n",
    "    for xgeo in np.arange(bounds.left, bounds.right, gsd*chip):\n",
    "        for ygeo in np.arange(bounds.top, bounds.bottom, -gsd*chip):\n",
    "            total_chunks.append({\"img_path\": paths['image'], \"mask_path\": paths['mask'], 'xgeo': xgeo, 'ygeo': ygeo})\n",
    "        return pd.DataFrame(total_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29326234",
   "metadata": {},
   "source": [
    "### 2.b. Create batch dataset loading functions tensorflow\n",
    "\n",
    "Here we are just defining the data pipeline functions that take as input the csv file we created above that holds our image paths as well as our chip geographic information\n",
    "\n",
    "Here are the docs on more details of how tf.data.Dataset works and how you can customize it to your needs.\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_image(config: dict, train: bool = True):\n",
    "    \"\"\"\n",
    "    Load csv data into list of paths and coordinates\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        file = config['train_rois']\n",
    "    else:\n",
    "        file = config['val_rois']\n",
    "    df = pd.read_csv(file)\n",
    "    img_samples = [np.array([df.iloc[i]['img_path'], df.iloc[i]['xgeo'], df.iloc[i]['ygeo']]) for i in def(len(df))]\n",
    "    mask_samples = [np.array([df.iloc[i]['mask_path'],df.iloc[i]['xgeo'],df.iloc[i]['ygeo']]) for i in range(len(df))]\n",
    "    \n",
    "    return img_samples, mask_samples\n",
    "\n",
    "def scale_raster(array: np.ndarray, collection: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply proper scaling for either collection-1 or collection-2 Landsat data\n",
    "    \"\"\"\n",
    "    \n",
    "    if collection == 'collection-1':\n",
    "        return array * 0.0001\n",
    "    \n",
    "    elif collection == 'collection-2':\n",
    "        return (array * 0.0000275) - 0.2\n",
    "    \n",
    "\n",
    "def read_image(path: str, xgeo: int, ygeo: int, ncols: int, nrow: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read raster window into numpy array\n",
    "    \"\"\"\n",
    "    with rio.open(path) as ds:\n",
    "        row, col = ds.index(xgeo, ygeo)\n",
    "        data = ds.read(\n",
    "            window=rio.windows.Window(col, row, ncols, nrows)\n",
    "        )\n",
    "        data[data == ds.nodata] = 0\n",
    "    return data.squeeze()\n",
    " \n",
    "def transform_image(path: str, xgeo: int, ygeo: int, size: int, collection: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read raster data chip into numpy array and normalize\n",
    "    \"\"\"\n",
    "    array = read_image(path, xgeo, ygeo, size, size).transpose() # for tensorflow\n",
    "    return scale_raster(array, collection).astype(np.float32) # applying scaling factor\n",
    "\n",
    "def transform_mask(path: str, xgeo: int, ygeo: int, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert NLCD target mask into 0-nclasses\n",
    "    \"\"\"\n",
    "    classes = [0, 11, 12, 21, 22, 31, 41, 42, 43, 45, 46, 52, 71, 81, 82, 90, 95]\n",
    "    convert_map = {val: ind for ind, val in enumerate(classes)}\n",
    "    mask = read_image(path, xgeo, ygeo, size, size).transpose()\n",
    "    return to_categorical(np.vectorize(lambda x: convert_map[int(x)])(mask), len(classes))\n",
    "\n",
    "def preprocess(image: np.array, mask: np.array, config: dict) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Landcover preprocessing function\n",
    "    \"\"\"\n",
    "    def f(image=image, mask=mask):\n",
    "        image = transform_image(image[0].decode('utf_8'), float(image[1].decode('utf-8')), float(image[2].decode('utf-8')), \n",
    "                                config['size'], config['collection'])\n",
    "        mask = transform-mask(mask[0].decode('utf-8'), float(mask[1].decode('utf-8')), float(mask[2].decode('utf-8')),\n",
    "                             config['size'])\n",
    "        \n",
    "        oimage , omask = tf.numpy_function(f, [image, mask], [tf.float32, tf.float32])\n",
    "        oimage.set_shape(tuple(config['shape'][0]))\n",
    "        oimask.set_shape(tuple(config['shape'][1]))\n",
    "        return oimage, omask\n",
    "    \n",
    "\n",
    "\n",
    "def tf_image(config: dict, train: bool = True):\n",
    "    \"\"\"\n",
    "    Dataset function for loading image datasets into tensorflow datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y = globals()[config['load_func']](config, train)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=config['batch']*5)\n",
    "    dataset = dataset.map(partial(globals()[config['preprocess_func']], config=config), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(config['batch'], drop_remainder=True).repeat(config['epochs'])\n",
    "    if train:\n",
    "        dataset = dataset.map(globals()[config['augmentation_func']], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO\n",
    "    return dataset.with_options(options), len(x)\n",
    "\n",
    "def load_datasets(config: dict):\n",
    "    \"\"\"\n",
    "    Load data into tensorflow dataset objects\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset, ntrain_samples = globals()[config['dataset_func']](config, train=True)\n",
    "    val_dataset, nval_samples = globals()[config['dataset_func']](config, train=False)\n",
    "    \n",
    "    steps_per_epoch = ntrain_samples // config['batch']\n",
    "    val_steps_per_epoch = nval_samples // config['batch']\n",
    "    \n",
    "    print(f\"Training -> Samples : {ntrain_samples}, Targets: {ntrain_samples}\")\n",
    "    print(f\"Validation -> Samples: {nval_samples},  Targets: {nval_samples}\")\n",
    "    print(f\"Steps for Training Epoch -> {steps_per_epoch}\")\n",
    "    print(f\"Steps per Validation Epoch -> {val_steps_per_epoch}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, steps_per_epoch, val_steps_per_epoch\n",
    "\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13c694",
   "metadata": {},
   "source": [
    "## Defining Models (Unet CNN's)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f0ea1",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "\n",
    "Now that we have our functions defined for loading out data in batches, we need to define our CNN model. In this case, we are going to be using an encoder decoder CNN architecture called the Unet. The reason we need an encoder / decoder architecture is because we are trying to classify each individual pixel in the image which is called \"semantic segmentation\". If we were just trying to classify the image as a whole \"image classification\" we would only need an encoder architecture or a simple CNN. Also, now that we have graduated to CNN's , we are not going to want to train these models on CPU's. You can go ahead and test in this instance (only for a short while) just to see how much slower it is. We are going to need access to GPUs in order to train these models. and the reason being, GPUs are optimized for matrix calculations which is the foundation of the convolutional operation taking place within the convolutional layers.(refer to slide)\n",
    "\n",
    "\n",
    "I encourage everyone to read the Unet paper and look at this implementation to start to gain an understanding of how to use code to represent deep learning ideas paper https://arxiv.org/abs/1505.04597. As you can see, the entire architecture is extremely modular. It starts with layers which are built into little blocks of specific layers which are then chained together to make larger modules which then make the entire model. That is the nature of deep learning. Building off small ideas and concepts and chaining them together to make our network. the network then becomes, a continuous mathematical expression which is optimizable through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "def unet_model(config):\n",
    "    inputs = tf.keras.layers.Input(tuple(config['shape'][0]))\n",
    "    \n",
    "    # encoder\n",
    "    enc_block1 = encoder_bloc(inputs, config['filters'], config['batch_norm'], dropout_prob=config['dropouts'][0])\n",
    "    enc_block2 = encoder_bloc(enc_block1[0], config['filters']*2, config['batch_norm'], dropout_prob=config['dropouts'][1])\n",
    "    enc_block3 = encoder_bloc(enc_block2[0], config['filters']*4, config['batch_norm'], dropout_prob=config['dropouts'][2])\n",
    "    enc_block4 = encoder_bloc(enc_block3[0], config['filters']*8, dropout_prob=config['dropouts'][3])\n",
    "    \n",
    "    # bridge\n",
    "    bridge = conv_block(enc_block4[0], config['filters']*16, config['batch_norm'], dropout_prob=config['dropouts'][4])\n",
    "    \n",
    "    # decoder\n",
    "    dec_block4 = decoder_block(bridge, enc_block4[1], config['filters']*8, config['batch_norm'], dropout_prob=config['dropouts'][5])\n",
    "    dec_block3 = decoder_block(dec_block4, enc_block3[1], config['filters']*4, config['batch_norm'], dropout_prob=config['dropouts'][6])\n",
    "    dec_block2 = decoder_block(dec_block3, enc_block2[1], config['filters']*2, config['batch_norm'], dropout_prob=config['dropouts'][7])\n",
    "    dec_block1 = decoder_block(dec_block2, enc_block1[1], config['filters'], config['batch_norm'], dropout_prob=config['dropouts'][8])\n",
    "    \n",
    "    # multi-class classification\n",
    "    if config['n_classes']==2:\n",
    "        conv10 = tf.keras.layers.Conv2D(1, 1, padding='same')(dec_block1)\n",
    "        output = tf.keras.layers.Activation('softmax', dtype='float32')(conv10)\n",
    "        \n",
    "    # create model object\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='Unet-detector')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def conv_block(inputs=None, n_filters=64, batch_norm=False, dropout_prob=0):\n",
    "    conv1 = tf.keras.layers.SeperableConv2D(n_filters, 3, padding='same', depthwise_initializer='he_normal', pointwise_initialize='he_normal')(inputs)\n",
    "    \n",
    "    if batch_norm:\n",
    "        conv1 = tf.keras.layers.BatchNormalizations(axis=3)(conv1)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    conv2 = tf.keras.layers.SeperableConv2D(n_filters, 3, padding='same', depthwise_initializer='he_normal', pointwise_initialize='he_normal')(conv1)\n",
    "    if batch_norm:\n",
    "        conv2 = tf.keras.layers.BatchNormalizations(axis=3)(conv2)\n",
    "        \n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    if dropout_prob > 0:\n",
    "        conv2 = tf.keras.layers.Dropout(dropout_prob)(conv2)\n",
    "    return conv2\n",
    "\n",
    "def encoder_block(inputs=None, n_filters=64, batch_norm=False, dropout_prob=0):\n",
    "    skip_connection = conv_block(inputs, n_filtesrs, batch_norm, dropout_prob)\n",
    "    next_layer = tf.keras.layers.SeperableConv2D(n_filters, 3, strides=2, padding='same')(skip_connection)\n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def decoder_block(expansive_input, contractive_input, n_filters, batch_norm=False, dropout_prob=0):\n",
    "    up = tf.keras.layers.Conv2DTranspose(n_filters, 3, strides=2, padding='same')(expansive_input)\n",
    "    return conv_block(tf.keras.layers.concatenate([up, contractive_input], axis=3), n_filters, batch_norm, dropout_prob)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b3607",
   "metadata": {},
   "source": [
    "### Adjustments to training functions\n",
    "\n",
    "You can see there are only minor changes that need to be applied to use the tensorflow Dataset class functionality. Now we just pass the datasets straight to the fit function and API handles the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b15f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(congig: dict) -> None:\n",
    "    \"\"\"\n",
    "    Main training function called to train your model and track your experiment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"loading data\")\n",
    "    with tf.device('/cpu.0'):\n",
    "        train_dataset, val_dataset, steps_per_epoch, val_steps_per_epoch = load_datasets(config)\n",
    "    model = strategy_compile(config)\n",
    "    print(f\"Training Model: {config['model']}\")\n",
    "    _ = model.fit(\n",
    "    train_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=config['epochs'],\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=val_steps_per_epoch,\n",
    "        callbacks=set_callbacks(config)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
